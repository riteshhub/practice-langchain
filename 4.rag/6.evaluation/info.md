## Evaluation of RAG Application

Evaluating a RAG system helps detect and prevent issues like hallucinations (incorrect or ungrounded responses), measure how well both the retrieval and generation components contribute to the output, and optimize configurations for the best results.

## ragas score

### retrieval

- **context precision** : the signal to noise ratio of retrieved context
  - Input
    * _Query_: The user question or input.
    * _Retrieved Contexts_: The list of text chunks (documents, sentences, passages) retrieved by the RAG system to answer the query.
    * _Reference (Ground Truth)_: The ideal, correct answer or information related to the query (sometimes called the ground-truth answer) .

- **context recall** : can it retrieve all the relevant information required to answer the question
  - Input
    * _Query_: The user question or input.
    * _Retrieved Contexts_: The list of text chunks (documents, sentences, passages) retrieved by the RAG system to answer the query.
    * _Reference (Ground Truth)_: The ideal, correct answer or information related to the query (sometimes called the ground-truth answer) .

### generation

- **faithfulness** : how factually accurate is the generated answer
  - Input
    * _Generate Answer_: The output generated by LLM.
    * _Retrieved Contexts_: The list of text chunks (documents, sentences, passages) retrieved by the RAG system to answer the query.

- **answer relevancy** : how relevant is the generated answer to the question
  - Input
    * _Query_: The user question or input.
    * _Generate Answer_: The output generated by LLM.
    * _Retrieved Contexts_: The list of text chunks (documents, sentences, passages) retrieved by the RAG system to answer the query.

## References
https://docs.ragas.io/en/v0.1.21/concepts/metrics/index.html
https://www.comet.com/docs/opik/evaluation/metrics/overview
